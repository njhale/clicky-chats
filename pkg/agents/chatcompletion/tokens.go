package chatcompletion

import (
	"encoding/json"
	"fmt"
	"strings"

	"github.com/gptscript-ai/clicky-chats/pkg/generated/openai"
	"github.com/pkoukk/tiktoken-go"
	"gorm.io/datatypes"
)

// countPromptTokens returns an estimate of the number of prompt tokens that will be generated by an OpenAI model
// for a set of messages.
func countPromptTokens(model string, messages datatypes.JSONSlice[openai.ChatCompletionRequestMessage]) (int, error) {
	tkm, err := tiktoken.EncodingForModel(model)
	if err != nil {
		return 0, fmt.Errorf("failed to get encoding for model %s: %w", model, err)
	}

	var tokensPerMessage, tokensPerName int
	switch model {
	case "gpt-3.5-turbo-0613",
		"gpt-3.5-turbo-16k-0613",
		"gpt-4-0314",
		"gpt-4-32k-0314",
		"gpt-4-0613",
		"gpt-4-32k-0613":
		tokensPerMessage = 3
		tokensPerName = 1
	case "gpt-3.5-turbo-0301":
		tokensPerMessage = 4 // every message follows <|start|>{role/name}\n{content}<|end|>\n
		tokensPerName = -1   // if there's a name, the role is omitted
	default:
		if strings.Contains(model, "gpt-3.5-turbo") {
			// gpt-3.5-turbo may update over time. Returning num tokens assuming gpt-3.5-turbo-0613.
			return countPromptTokens("gpt-3.5-turbo-0613", messages)
		}
		if strings.Contains(model, "gpt-4") {
			// gpt-4 may update over time. Returning num tokens assuming gpt-4-0613.
			return countPromptTokens("gpt-4-0613", messages)
		}

		return 0, fmt.Errorf("token counting method for model %s is unknown", model)
	}

	msgs, err := toPromptMessages(messages)
	if err != nil {
		return 0, fmt.Errorf("failed to convert chat completion messages to prompt messages: %w", err)
	}

	var tokens int
	for _, msg := range msgs {
		tokens += tokensPerMessage
		tokens += len(tkm.Encode(msg.Content, nil, nil))
		tokens += len(tkm.Encode(msg.Role, nil, nil))
		tokens += len(tkm.Encode(msg.Name, nil, nil))
		if msg.Name != "" {
			tokens += tokensPerName
		}
	}

	return tokens, nil
}

type promptMessage struct {
	Content string `json:"content"`
	Name    string `json:"name"`
	Role    string `json:"role"`
}

func toPromptMessages(from datatypes.JSONSlice[openai.ChatCompletionRequestMessage]) ([]promptMessage, error) {
	data, err := json.Marshal(from)
	if err != nil {
		return nil, err
	}

	to := make([]promptMessage, 0)
	return to, json.Unmarshal(data, &to)
}
